# sophie-s_world_rag

# ğŸ“˜ Sophie's World â€“ RAG (Retrieval Augmented Generation)

This project is a **Retrieval Augmented Generation (RAG)** system built on top of the novel **"Sophie's World" by Jostein Gaarder**.

The system allows users to ask questions about the book and get answers **strictly grounded in the novel text**, using vector search and an LLM.

---

## âœ¨ Features

* ğŸ“š Uses **Sophie's World** as the single source of truth
* ğŸ” Semantic search using **ChromaDB** (vector database)
* ğŸ§  Multiple retrieval strategies:

  * Similarity Retriever
  * MMR Retriever
  * Multi-Query Retriever
* ğŸ§© Custom chunking & preprocessing pipeline
* ğŸ§ª Hallucination-controlled prompt design
* ğŸ–¥ï¸ Interactive **Streamlit UI**
* ğŸ§± Clean `src/`-based project structure

---

## ğŸ—ï¸ Project Architecture

```
sophie's_world_rag/
â”‚
â”œâ”€â”€ app.py                  # Streamlit entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chains/serve_chain.py              # Chain factory (serve_rag_chain)
â”‚   â”œâ”€â”€ chians/chains.py            # RAG chain definitions
â”‚   â”œâ”€â”€ config/config.py            # Central configuration
â”‚   â”œâ”€â”€ prompts/prompts.py           # Prompt templates
â”‚   â””â”€â”€ utils/utils.py             # Helper functions (format_docs etc.)
â”‚
â”œâ”€â”€ data/                    # Raw & preprocessed novel data
â”œâ”€â”€ chroma_db/               # Chroma vector store
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ RAG Pipeline Overview

1. **Data Preprocessing**

   * Raw novel text is cleaned
   * Split into overlapping chunks
   * Chunk size & overlap configurable

2. **Embedding Generation**

   * Each chunk converted to embeddings
   * Stored in **ChromaDB**

3. **Retrieval**

   * User query â†’ embedding
   * Top-k relevant chunks retrieved
   * Optional MMR / Multi-query expansion

4. **Generation**

   * Retrieved chunks injected into prompt
   * LLM generates answer using ONLY provided context

---

## ğŸ§  Prompt Philosophy

The system prompt is designed to:

* Treat retrieved chunks as the **primary source of truth**
* Prevent use of external or general knowledge
* Explicitly refuse answers when context is insufficient

This helps minimize hallucinations and keeps answers grounded in the novel.

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run Streamlit app

```bash
streamlit run app.py
```

Open browser at:

```
http://localhost:8501
```

---

## ğŸ§ª Example Questions

* Who is Sophie?
* Why does Sophie receive mysterious letters?
* What does philosophy mean in the early chapters?

If the answer is not present in the retrieved excerpts, the system will clearly say so.

---

## âš™ï¸ Configuration

All major settings are controlled via `config.py`:

* Embedding model
* LLM model
* Chunk size & overlap
* Retriever strategy

This allows easy experimentation without touching core logic.

---

## ğŸš€ Future Improvements

* Source citations (chapter / chunk id)
* Chat-style multi-turn memory
* Hybrid retriever (BM25 + vector)
* RAG evaluation (RAGAS)
* Dockerized deployment

---

## ğŸ“Œ Notes

* `.venv` is excluded via `.gitignore`
* Project follows production-style Python packaging
* Designed for learning, experimentation, and extension

---

## ğŸ™Œ Author

Built as a hands-on RAG learning project using **Sophie's World** as a knowledge base.

---

Happy exploring philosophy with RAG ğŸš€
